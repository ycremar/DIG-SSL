{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of GraphCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sslgraph.utils import Encoder, EvalSemisupevised, EvalUnsupevised, get_dataset\n",
    "from sslgraph.contrastive.model import GraphCL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Semi-supervised learning\n",
    "\n",
    "#### Load dataset\n",
    "\n",
    "In this example, we evaluate model on NCI1 dataset in the semi-supervised setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataset_pretrain = get_dataset('DD', task='semisupervised')\n",
    "feat_dim = dataset[0].x.shape[1]\n",
    "embed_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your encoder and contrastive model (GraphCL)\n",
    "\n",
    "For semi-supervised setting, GraphCL uses ResGCN. \n",
    "\n",
    "Available augmentation includes: dropN, maskN, permE, subgraph, random[2-4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(feat_dim, embed_dim, n_layers=3, gnn='resgcn')\n",
    "graphcl = GraphCL(embed_dim, aug_1='random2', aug_2='random2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define evaluatior instance\n",
    "\n",
    "In this example, we use a label rate of 1%.\n",
    "\n",
    "To setup configurations (num of epochs, learning rates, etc. for pretraining and finetuning), run\n",
    "\n",
    "\n",
    "`evaluator.setup_train_config(batch_size = 128,\n",
    "    p_optim = 'Adam', p_lr = 0.0001, p_weight_decay = 0, p_epoch = 100,\n",
    "    f_optim = 'Adam', f_lr = 0.001, f_weight_decay = 0, f_epoch = 100)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = EvalSemisupevised(dataset, dataset_pretrain, label_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform evaluation\n",
    "\n",
    "You can also perform evaluation with grid search on pre-training epoch and\n",
    "learning rate by running\n",
    "``\n",
    "evaluator.grid_search(learning_model=graphcl, encoder=encoder, \n",
    "    p_lr_lst=[0.1,0.01,0.001,0.0001], p_epoch_lst=[20,40,60,80,100])\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretraining: epoch 20: 100%|██████████| 20/20 [03:53<00:00, 11.67s/it, loss=2.863035]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.40it/s, acc=0.6864, val_loss=1.1754]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.70it/s, acc=0.7542, val_loss=0.9923]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.29it/s, acc=0.6356, val_loss=1.7539]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.19it/s, acc=0.7542, val_loss=0.9935]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.25it/s, acc=0.8051, val_loss=0.9999]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.47it/s, acc=0.7203, val_loss=1.1457]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.47it/s, acc=0.6695, val_loss=1.3771]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.33it/s, acc=0.7627, val_loss=0.8442]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.76it/s, acc=0.7094, val_loss=1.7390]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.17it/s, acc=0.7692, val_loss=0.8264]\n",
      "Pretraining: epoch 40: 100%|██████████| 40/40 [06:49<00:00, 10.23s/it, loss=3.187325]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:22<00:00,  4.37it/s, acc=0.7034, val_loss=1.1999]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:23<00:00,  4.28it/s, acc=0.6949, val_loss=1.5845]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:22<00:00,  4.43it/s, acc=0.6525, val_loss=1.0084]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:21<00:00,  4.71it/s, acc=0.7119, val_loss=0.5771]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:13<00:00,  7.68it/s, acc=0.7034, val_loss=0.5687]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.39it/s, acc=0.7203, val_loss=0.6121]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.11it/s, acc=0.6441, val_loss=1.3351]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.34it/s, acc=0.6356, val_loss=1.1120]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.76it/s, acc=0.6581, val_loss=1.2831]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.37it/s, acc=0.7436, val_loss=0.8041]\n",
      "Pretraining: epoch 60: 100%|██████████| 60/60 [10:06<00:00, 10.11s/it, loss=2.883049]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:26<00:00,  3.77it/s, acc=0.7034, val_loss=0.7423]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:21<00:00,  4.71it/s, acc=0.7458, val_loss=0.5311]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:25<00:00,  3.92it/s, acc=0.6610, val_loss=1.8823]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:11<00:00,  8.54it/s, acc=0.7627, val_loss=0.6232]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:08<00:00, 11.17it/s, acc=0.7288, val_loss=1.7666]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.76it/s, acc=0.7797, val_loss=0.4811]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.57it/s, acc=0.7966, val_loss=0.5331]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.75it/s, acc=0.6780, val_loss=1.8362]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.62it/s, acc=0.6581, val_loss=12.0736]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.21it/s, acc=0.6752, val_loss=0.6108]\n",
      "Pretraining: epoch 80: 100%|██████████| 80/80 [13:48<00:00, 10.35s/it, loss=2.730827]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:24<00:00,  4.00it/s, acc=0.6525, val_loss=0.6775]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:23<00:00,  4.28it/s, acc=0.6525, val_loss=0.8563]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:51<00:00,  1.96it/s, acc=0.7542, val_loss=1.2074]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:38<00:00,  2.58it/s, acc=0.7542, val_loss=0.5734]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:13<00:00,  7.55it/s, acc=0.6864, val_loss=1.1458]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:11<00:00,  8.91it/s, acc=0.6780, val_loss=1.2448]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.22it/s, acc=0.8559, val_loss=0.5027]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.53it/s, acc=0.6017, val_loss=2.4688]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.39it/s, acc=0.6838, val_loss=1.3313]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:10<00:00,  9.77it/s, acc=0.7778, val_loss=0.6212]\n",
      "Pretraining: epoch 100: 100%|██████████| 100/100 [13:47<00:00,  8.27s/it, loss=2.871308]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.40it/s, acc=0.6271, val_loss=0.8962]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.92it/s, acc=0.6864, val_loss=0.7160]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.12it/s, acc=0.6864, val_loss=6.8170]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.38it/s, acc=0.7288, val_loss=0.6063]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:08<00:00, 11.27it/s, acc=0.7373, val_loss=0.6155]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.23it/s, acc=0.7458, val_loss=10.9779]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.30it/s, acc=0.8220, val_loss=0.6966]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.46it/s, acc=0.6525, val_loss=2.0407]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.52it/s, acc=0.6581, val_loss=2.4203]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.06it/s, acc=0.7350, val_loss=0.6181]\n",
      "Pretraining: epoch 20: 100%|██████████| 20/20 [02:30<00:00,  7.51s/it, loss=2.896626]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.02it/s, acc=0.6780, val_loss=1.4893]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 11.01it/s, acc=0.6864, val_loss=0.6474]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.24it/s, acc=0.7119, val_loss=0.7283]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.33it/s, acc=0.7627, val_loss=0.5696]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:09<00:00, 11.07it/s, acc=0.7119, val_loss=0.6195]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.50it/s, acc=0.7627, val_loss=0.5189]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:10<00:00,  9.96it/s, acc=0.7627, val_loss=0.9844]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.79it/s, acc=0.6695, val_loss=1.1639]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.91it/s, acc=0.7009, val_loss=38.2286]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.27it/s, acc=0.7607, val_loss=0.5774]\n",
      "Pretraining: epoch 40: 100%|██████████| 40/40 [05:02<00:00,  7.56s/it, loss=2.863725]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.36it/s, acc=0.6695, val_loss=0.7838]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.65it/s, acc=0.6102, val_loss=0.7165]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.25it/s, acc=0.7203, val_loss=0.9410]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.08it/s, acc=0.7034, val_loss=0.5976]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:08<00:00, 11.12it/s, acc=0.7458, val_loss=0.7463]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.57it/s, acc=0.7627, val_loss=0.6190]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.23it/s, acc=0.7966, val_loss=0.5817]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.41it/s, acc=0.6864, val_loss=1.1195]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.55it/s, acc=0.6667, val_loss=72.3172]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.10it/s, acc=0.8034, val_loss=0.5995]\n",
      "Pretraining: epoch 60: 100%|██████████| 60/60 [07:32<00:00,  7.54s/it, loss=3.226601]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.47it/s, acc=0.6525, val_loss=0.7546]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.99it/s, acc=0.6102, val_loss=1.8031]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.33it/s, acc=0.6780, val_loss=1.2186]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.32it/s, acc=0.7627, val_loss=0.5651]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:08<00:00, 11.26it/s, acc=0.6441, val_loss=1834.4011]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.54it/s, acc=0.6949, val_loss=0.6533]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.33it/s, acc=0.7881, val_loss=0.6189]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.69it/s, acc=0.6525, val_loss=1.3884]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.54it/s, acc=0.7179, val_loss=3.8940]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:10<00:00,  9.98it/s, acc=0.7863, val_loss=0.5869]\n",
      "Pretraining: epoch 80: 100%|██████████| 80/80 [09:57<00:00,  7.47s/it, loss=2.744104]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.38it/s, acc=0.6780, val_loss=0.8261]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.91it/s, acc=0.6864, val_loss=0.7373]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.03it/s, acc=0.7034, val_loss=5.8112]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.51it/s, acc=0.7203, val_loss=0.5322]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:08<00:00, 11.24it/s, acc=0.6780, val_loss=0.8642]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.70it/s, acc=0.7373, val_loss=0.5571]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.25it/s, acc=0.7966, val_loss=0.5897]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.60it/s, acc=0.6695, val_loss=2.7950]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.36it/s, acc=0.7607, val_loss=1.3589]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.17it/s, acc=0.7265, val_loss=0.5932]\n",
      "Pretraining: epoch 100: 100%|██████████| 100/100 [12:31<00:00,  7.51s/it, loss=2.522811]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.28it/s, acc=0.6695, val_loss=0.9816]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.93it/s, acc=0.6780, val_loss=0.8489]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.43it/s, acc=0.7034, val_loss=0.9718]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.23it/s, acc=0.7203, val_loss=0.5556]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.94it/s, acc=0.7119, val_loss=0.7007]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.49it/s, acc=0.7542, val_loss=2.5679]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.17it/s, acc=0.7627, val_loss=0.5815]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.54it/s, acc=0.6780, val_loss=1.1523]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.65it/s, acc=0.7265, val_loss=2.8650]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.33it/s, acc=0.7265, val_loss=0.6712]\n",
      "Pretraining: epoch 20: 100%|██████████| 20/20 [02:29<00:00,  7.47s/it, loss=2.912696]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:10<00:00,  9.90it/s, acc=0.6695, val_loss=1.4582]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.76it/s, acc=0.6695, val_loss=1.5330]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.40it/s, acc=0.6949, val_loss=0.9944]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.37it/s, acc=0.7203, val_loss=0.6615]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:08<00:00, 11.21it/s, acc=0.7288, val_loss=0.8344]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.40it/s, acc=0.7627, val_loss=0.4895]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.11it/s, acc=0.7542, val_loss=0.5820]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.63it/s, acc=0.6949, val_loss=1.1705]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.65it/s, acc=0.7179, val_loss=2.4369]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.19it/s, acc=0.7692, val_loss=0.6207]\n",
      "Pretraining: epoch 40: 100%|██████████| 40/40 [04:59<00:00,  7.49s/it, loss=2.829732]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.36it/s, acc=0.6695, val_loss=0.8383]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 11.00it/s, acc=0.6864, val_loss=1.2490]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.15it/s, acc=0.7288, val_loss=0.8617]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.41it/s, acc=0.7373, val_loss=0.6036]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:08<00:00, 11.28it/s, acc=0.7119, val_loss=0.6570]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.36it/s, acc=0.7203, val_loss=0.5116]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.34it/s, acc=0.7797, val_loss=0.5534]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.54it/s, acc=0.6780, val_loss=1.1251]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.29it/s, acc=0.7692, val_loss=2.3859]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.07it/s, acc=0.7179, val_loss=0.6366]\n",
      "Pretraining: epoch 60: 100%|██████████| 60/60 [07:29<00:00,  7.50s/it, loss=2.733662]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.41it/s, acc=0.6949, val_loss=0.9320]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 11.06it/s, acc=0.6610, val_loss=0.6921]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.24it/s, acc=0.6186, val_loss=1.0410]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.51it/s, acc=0.7627, val_loss=0.5644]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.99it/s, acc=0.6949, val_loss=7.3533]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.51it/s, acc=0.7203, val_loss=1.3722]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.10it/s, acc=0.8051, val_loss=0.5323]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.78it/s, acc=0.6695, val_loss=1.1491]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.68it/s, acc=0.6581, val_loss=1.3599]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.13it/s, acc=0.7436, val_loss=0.6866]\n",
      "Pretraining: epoch 80: 100%|██████████| 80/80 [09:58<00:00,  7.48s/it, loss=2.725834]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.36it/s, acc=0.6695, val_loss=0.7878]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.82it/s, acc=0.6864, val_loss=0.7114]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.38it/s, acc=0.7203, val_loss=0.8188]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.15it/s, acc=0.7288, val_loss=0.5941]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:09<00:00, 11.08it/s, acc=0.7542, val_loss=0.6030]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.40it/s, acc=0.7712, val_loss=0.7183]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.20it/s, acc=0.8136, val_loss=0.5551]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.71it/s, acc=0.6949, val_loss=1.2046]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.35it/s, acc=0.7607, val_loss=2.1468]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.08it/s, acc=0.7778, val_loss=0.5850]\n",
      "Pretraining: epoch 100: 100%|██████████| 100/100 [12:27<00:00,  7.47s/it, loss=2.715357]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.41it/s, acc=0.6780, val_loss=1.4356]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.61it/s, acc=0.6949, val_loss=1.1602]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:10<00:00,  9.96it/s, acc=0.7034, val_loss=1.0688]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.39it/s, acc=0.7119, val_loss=0.6383]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.72it/s, acc=0.7119, val_loss=0.9679]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.60it/s, acc=0.7712, val_loss=0.5160]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.24it/s, acc=0.7712, val_loss=0.6356]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.73it/s, acc=0.7034, val_loss=1.4665]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.61it/s, acc=0.7692, val_loss=2.6031]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.03it/s, acc=0.7350, val_loss=0.6300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best paras: 20 epoch, lr=0.010000, acc=0.7631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7631392478942871, 0.030191147699952126, (0.01, 20))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.grid_search(learning_model=graphcl, encoder=encoder,\n",
    "                      p_lr_lst=[0.01,0.001,0.0001], p_epoch_lst=[20,40,60,80,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reproduce results in the paper, you may want to perform grid search and run evaluation for 5 times and take the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example with a label rate of 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretraining: epoch 100: 100%|██████████| 100/100 [12:27<00:00,  7.47s/it, loss=2.694387]\n",
      "Fold 1, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.11it/s, acc=0.7119, val_loss=1.5941]\n",
      "Fold 2, finetuning: 100%|██████████| 100/100 [00:09<00:00, 11.04it/s, acc=0.7119, val_loss=1.2151]\n",
      "Fold 3, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.53it/s, acc=0.7203, val_loss=1.7954]\n",
      "Fold 4, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.53it/s, acc=0.7203, val_loss=1.0913]\n",
      "Fold 5, finetuning: 100%|██████████| 100/100 [00:09<00:00, 11.08it/s, acc=0.7542, val_loss=1.3340]\n",
      "Fold 6, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.73it/s, acc=0.7458, val_loss=1.1896]\n",
      "Fold 7, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.21it/s, acc=0.8305, val_loss=1.0420]\n",
      "Fold 8, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.46it/s, acc=0.7034, val_loss=1.3943]\n",
      "Fold 9, finetuning: 100%|██████████| 100/100 [00:09<00:00, 10.39it/s, acc=0.6752, val_loss=1.6866]\n",
      "Fold 10, finetuning: 100%|██████████| 100/100 [00:10<00:00,  9.97it/s, acc=0.7009, val_loss=0.9911]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7554904222488403, 0.038286566734313965)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(feat_dim, embed_dim, n_layers=3, gnn='resgcn')\n",
    "graphcl = GraphCL(embed_dim, aug_1='random2', aug_2='random2')\n",
    "evaluator = EvalSemisupevised(dataset, dataset_pretrain, label_rate=0.1)\n",
    "evaluator.evaluate(learning_model=graphcl, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Unsupervised representation learning\n",
    "\n",
    "#### Load dataset\n",
    "\n",
    "In this example, we evaluate model on MUTAG dataset in the unsupervised setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset('MUTAG', task='unsupervised', feat_str='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your encoder and contrastive model (GraphCL)\n",
    "\n",
    "For unsupervised setting, GraphCL uses GIN with jumping knowledge (with output_dim = hidden_dim * n_layers). \n",
    "\n",
    "Available augmentation includes: dropN, maskN, permE, subgraph, random[2-4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "encoder = Encoder(feat_dim=dataset[0].x.shape[1], hidden_dim=embed_dim, n_layers=3, gnn='gin')\n",
    "graphcl = GraphCL(embed_dim*3, aug_1=None, aug_2='random2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform evaluation with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretraining: epoch 20: 100%|██████████| 20/20 [00:06<00:00,  3.09it/s, loss=5.231109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch 20: acc 0.8626 +/-(0.0615)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8625730994152047, 0.061473087841019416)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = EvalUnsupevised(dataset, log_interval=20)\n",
    "evaluator.evaluate(learning_model=graphcl, encoder=encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
